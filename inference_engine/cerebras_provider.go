// /home/gperry/Documents/GitHub/cloud-equities/FIG_Inference/inference/cerebras_provider.go

package inference_engine

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log"
	"net/http"

	// "strconv" // Removed if not used directly
	"strings"
	"sync"

	// Use official gollm imports
	"github.com/guiperry/gollm_cerebras/config" // For SetDefaultOptions
	"github.com/guiperry/gollm_cerebras/providers"
	gollm_types "github.com/guiperry/gollm_cerebras/types" // Renamed import to avoid conflict
	"github.com/guiperry/gollm_cerebras/utils"             // For Logger

	// Ordered map for JSON schema properties
	orderedmap "github.com/wk8/go-ordered-map/v2"

	// Import jsonschema for tool parameters and response format
	"github.com/invopop/jsonschema"
)

// These only need the fields/methods actually used by CerebrasProvider.

// Messages is likely a slice of Message pointers or values
type Messages []*Message

// Validatable is likely an interface, but we might not need its methods.
// Define as an empty interface for now if only type checking is needed.
type Validatable interface {
	// Add methods here ONLY if CerebrasProvider calls them.
	// Validate() error // Example
}

// ChatResult likely holds the final response message(s).
type ChatResult struct {
	Messages []*Message
	// Add other fields if CerebrasProvider uses them from ToResult()
}

// MessageFragment mimics genai.MessageFragment for streaming.
type MessageFragment struct {
	Content string
	// Add other fields if ParseStreamResponse needs them
}

// Local Time type mimicking genai.Time if needed for response parsing
// (Though ChatCompletionResponse uses int64 directly, which is fine)
// type Time int64
// func (t *Time) AsTime() time.Time { return time.Unix(int64(*t), 0) }

// --- Cerebras API Specific Structs (Based on genai documentation) ---
// These remain as defined before, using local types where needed

// CerebrasMessage matches the structure expected by the Cerebras API request/response.
type CerebrasMessage struct {
	Role       string             `json:"role,omitempty"` // "system", "assistant", "user", "tool"
	Content    string             `json:"content,omitempty"`
	ToolCalls  []CerebrasToolCall `json:"tool_calls,omitempty"`
	ToolCallID string             `json:"tool_call_id,omitempty"` // Used for tool response messages
}

// CerebrasTool defines the structure for a tool available to the Cerebras model.
type CerebrasTool struct {
	Type     string `json:"type"` // Should be "function"
	Function struct {
		Name        string             `json:"name"`
		Description string             `json:"description,omitempty"`
		Parameters  *jsonschema.Schema `json:"parameters"` // Using jsonschema type
	} `json:"function"`
}

// CerebrasToolCall represents a tool call requested by the model or provided in a response.
type CerebrasToolCall struct {
	Type     string `json:"type,omitempty"`  // "function"
	ID       string `json:"id,omitempty"`    // ID generated by the model for the call
	Index    *int64 `json:"index,omitempty"` // Optional index
	Function struct {
		Name      string `json:"name,omitempty"`
		Arguments string `json:"arguments,omitempty"` // Arguments are a JSON string
	} `json:"function,omitempty"`
}

// ChatCompletionRequest matches the Cerebras API request structure.
type ChatCompletionRequest struct {
	Model          string            `json:"model"`
	Messages       []CerebrasMessage `json:"messages"` // Use CerebrasMessage
	MaxTokens      int               `json:"max_tokens,omitempty"`
	ResponseFormat *struct {
		Type       string `json:"type"`
		JSONSchema *struct {
			Name   string             `json:"name"`
			Schema *jsonschema.Schema `json:"schema"`
			Strict bool               `json:"strict,omitempty"`
		} `json:"json_schema,omitempty"`
	} `json:"response_format,omitempty"`
	Seed        *int64         `json:"seed,omitempty"`
	Stop        []string       `json:"stop,omitempty"`
	Stream      bool           `json:"stream,omitempty"`
	Temperature *float64       `json:"temperature,omitempty"`
	TopP        *float64       `json:"top_p,omitempty"`
	ToolChoice  string         `json:"tool_choice,omitempty"`
	Tools       []CerebrasTool `json:"tools,omitempty"`
	User        string         `json:"user,omitempty"`
	Logprobs    bool           `json:"logprobs,omitempty"`
	TopLogprobs *int64         `json:"top_logprobs,omitempty"`
}

// ChatCompletionResponse matches the Cerebras API response structure.
type ChatCompletionResponse struct {
	ID      string `json:"id"`
	Object  string `json:"object"`
	Created int64  `json:"created"`
	Model   string `json:"model"`
	Choices []struct {
		Index        int64           `json:"index"`
		Message      CerebrasMessage `json:"message"`
		FinishReason string          `json:"finish_reason"`
	} `json:"choices"`
	Usage *struct {
		PromptTokens     int64 `json:"prompt_tokens"`
		CompletionTokens int64 `json:"completion_tokens"`
		TotalTokens      int64 `json:"total_tokens"`
	} `json:"usage,omitempty"`
}

// streamChoiceDelta matches the delta part of a streaming chunk.
type streamChoiceDelta struct {
	Role      string             `json:"role"`
	Content   string             `json:"content"`
	ToolCalls []CerebrasToolCall `json:"tool_calls"`
}

// streamChoice matches the choice part of a streaming chunk.
type streamChoice struct {
	Index        int64             `json:"index"`
	Delta        streamChoiceDelta `json:"delta"`
	FinishReason *string           `json:"finish_reason"`
}

// streamChunk matches the overall structure of a streaming chunk response.
type streamChunk struct {
	ID      string         `json:"id"`
	Object  string         `json:"object"`
	Created int64          `json:"created"`
	Model   string         `json:"model"`
	Choices []streamChoice `json:"choices"`
	Usage   *struct {
		PromptTokens     *int64 `json:"prompt_tokens,omitempty"`
		CompletionTokens *int64 `json:"completion_tokens,omitempty"`
	} `json:"usage,omitempty"`
}

// --- CerebrasProvider Implementation ---

// CerebrasProvider implements providers.Provider for the Cerebras API
type CerebrasProvider struct {
	apiKey       string
	model        string
	maxTokens    int
	temperature  *float64
	topP         *float64
	seed         *int64
	extraHeaders map[string]string
	logger       utils.Logger
	client       *http.Client // Changed from *CerebrasClient, gollm likely handles HTTP

	mutex sync.Mutex
}

// --- Registration ---
func init() {
	registry := providers.GetDefaultRegistry()
	// Ensure NewCerebrasProvider matches the expected ProviderConstructor signature
	registry.Register("cerebras", NewCerebrasProvider)
	log.Println("Registered Cerebras provider constructor with gollm registry")
}

// NewCerebrasProvider creates a new Cerebras provider instance.
// *** Changed signature BACK to accept arguments ***
// This matches the likely ProviderConstructor type:
// func(apiKey, model string, extraHeaders map[string]string) providers.Provider
func NewCerebrasProvider(apiKey, model string, extraHeaders map[string]string) providers.Provider {
	log.Printf("[DEBUG] NewCerebrasProvider called! apiKey: %t, model: %s", apiKey != "", model) // Log entry and basic args check
	// Initialize with arguments and defaults
	provider := &CerebrasProvider{
		apiKey:       apiKey, // Use provided apiKey
		model:        model,  // Use provided model
		maxTokens:    1000,   // Default max tokens (can be overridden)
		extraHeaders: make(map[string]string),
		logger:       utils.NewLogger(utils.LogLevelInfo),
		client:       &http.Client{},
	}
	// Set default model if provided one is empty
	if provider.model == "" {
		provider.model = "llama-4-scout-17b-16e-instruct"
	}
	// Copy provided extraHeaders
	for k, v := range extraHeaders {
		provider.extraHeaders[k] = v
	}
	log.Printf("NewCerebrasProvider created: model=%s", provider.model) // Add log
	return provider
}

// Name returns the provider's identifier.
func (p *CerebrasProvider) Name() string {
	return "cerebras"
}

// Endpoint returns the API endpoint URL.
func (p *CerebrasProvider) Endpoint() string {
	// Define CerebrasAPIURL if not defined elsewhere
	const CerebrasAPIURL = "https://api.cerebras.ai/v1/chat/completions"
	return CerebrasAPIURL
}

// Headers returns the necessary HTTP headers.
func (p *CerebrasProvider) Headers() map[string]string {
	p.mutex.Lock()
	defer p.mutex.Unlock()

	headers := map[string]string{
		"Content-Type": "application/json",
		"User-Agent":   "FIG-Inference/1.0 (via Gollm Provider)",
	}
	// apiKey is now set during construction
	if p.apiKey != "" {
		headers["Authorization"] = "Bearer " + p.apiKey
	} else {
		// Log warning if key is missing when headers are requested
		p.logger.Warn("Cerebras API key is missing when generating headers")
	}
	for k, v := range p.extraHeaders {
		headers[k] = v
	}
	return headers
}

// Helper to convert gollm messages to Cerebras messages
// Using gollm_types.MemoryMessage now
func convertMessagesToCerebras(messages []gollm_types.MemoryMessage) []CerebrasMessage {
	apiMessages := make([]CerebrasMessage, 0, len(messages))
	for _, msg := range messages {
		role := "user"
		switch strings.ToLower(msg.Role) {
		case "system", "assistant", "tool":
			role = strings.ToLower(msg.Role)
		case "ai":
			role = "assistant"
		}

		// Basic conversion - assumes text content primarily
		// TODO: Enhance if gollm_types.MemoryMessage includes structured tool call/result info
		apiMsg := CerebrasMessage{
			Role:    role,
			Content: msg.Content,
			// ToolCallID: // Map if msg represents a tool result
		}
		apiMessages = append(apiMessages, apiMsg)
	}
	return apiMessages
}

// Helper to convert gollm tools to Cerebras tools
func convertToolsToCerebras(gollmTools []utils.Tool) ([]CerebrasTool, error) {
	if len(gollmTools) == 0 {
		return nil, nil
	}
	apiTools := make([]CerebrasTool, 0, len(gollmTools))
	for _, tool := range gollmTools {
		if tool.Type != "function" {
			log.Printf("Warning: Skipping non-function tool type '%s'", tool.Type)
			continue
		}

		// Create default empty schema
		paramsSchema := &jsonschema.Schema{
			Type:       "object",
			Properties: orderedmap.New[string, *jsonschema.Schema](),
		}

		// Handle parameters if they exist
		if tool.Function.Parameters != nil {
			reflector := jsonschema.Reflector{}
			if schema := reflector.Reflect(tool.Function.Parameters); schema != nil {
				paramsSchema = schema
			} else {
				log.Printf("Warning: Failed to reflect jsonschema from tool parameters")
			}
		}

		apiTool := CerebrasTool{
			Type: "function",
			Function: struct {
				Name        string             `json:"name"`
				Description string             `json:"description,omitempty"`
				Parameters  *jsonschema.Schema `json:"parameters"`
			}{
				Name:        tool.Function.Name,
				Description: tool.Function.Description,
				Parameters:  paramsSchema,
			},
		}
		apiTools = append(apiTools, apiTool)
	}
	return apiTools, nil
}

// Helper to determine Cerebras tool choice string/object
// Returning interface{} as Cerebras might expect a struct for specific choice
func getCerebrasToolChoice(gollmChoice interface{}) interface{} {
	if gollmChoice == nil {
		return "auto" // Default to auto if nil
	}
	if choiceStr, ok := gollmChoice.(string); ok {
		choiceLower := strings.ToLower(choiceStr)
		if choiceLower == "auto" || choiceLower == "none" || choiceLower == "required" {
			return choiceLower // Return standard keywords as string
		}
		// If it's another string, assume it's a function name for specific choice
		log.Printf("Interpreting tool_choice string '%s' as specific function request.", choiceStr)
		return map[string]interface{}{ // Return the required struct
			"type": "function",
			"function": map[string]string{
				"name": choiceStr,
			},
		}
	}
	// Handle map choices (already in the correct struct format?)
	if choiceMap, ok := gollmChoice.(map[string]interface{}); ok {
		if typeVal, ok := choiceMap["type"].(string); ok && typeVal == "function" {
			if funcMap, ok := choiceMap["function"].(map[string]interface{}); ok {
				if _, ok := funcMap["name"].(string); ok {
					return choiceMap // Assume the map is already correct
				}
			}
		}
	}
	log.Printf("Warning: Unsupported tool_choice type: %T. Defaulting to 'auto'.", gollmChoice)
	return "auto" // Fallback to auto
}

// PrepareRequest creates the request body for a standard API call.
func (p *CerebrasProvider) PrepareRequest(prompt string, options map[string]interface{}) ([]byte, error) {
	p.mutex.Lock()
	model := p.model
	maxTokens := p.maxTokens
	apiKey := p.apiKey
	temp := p.temperature
	topP := p.topP
	seed := p.seed
	p.mutex.Unlock()

	req := ChatCompletionRequest{
		Model: model,
		Messages: []CerebrasMessage{
			{Role: "user", Content: prompt},
		},
		MaxTokens:   maxTokens,
		Temperature: temp,
		TopP:        topP,
		Seed:        seed,
	}

	// Apply Options
	if m, ok := options["model"].(string); ok && m != "" {
		req.Model = m
	}
	if mtVal, ok := options["max_tokens"]; ok {
		switch mt := mtVal.(type) {
		case int:
			if mt > 0 {
				req.MaxTokens = mt
			}
		case float64:
			if mt > 0 {
				req.MaxTokens = int(mt)
			}
		}
	}
	if tVal, ok := options["temperature"].(float64); ok {
		req.Temperature = &tVal
	}
	if pVal, ok := options["top_p"].(float64); ok {
		req.TopP = &pVal
	}
	if sVal, ok := options["seed"]; ok { // Handle int, int64, float64
		switch s := sVal.(type) {
		case int:
			seed64 := int64(s)
			req.Seed = &seed64
		case int64:
			req.Seed = &s
		case float64:
			seed64 := int64(s)
			req.Seed = &seed64
		}
	}
	if stopVal, ok := options["stop"].([]string); ok {
		req.Stop = stopVal
	}

	// Handle Tools & ToolChoice
	if toolsVal, ok := options["tools"].([]utils.Tool); ok {
		apiTools, err := convertToolsToCerebras(toolsVal)
		if err != nil {
			p.logger.Error("Failed to convert tools", "error", err)
		}
		req.Tools = apiTools
	}
	if toolChoiceVal, ok := options["tool_choice"]; ok {
		// Get the choice, which might be string or map
		choice := getCerebrasToolChoice(toolChoiceVal)
		// Marshal if it's a map, otherwise assign string directly
		if choiceStr, ok := choice.(string); ok {
			req.ToolChoice = choiceStr
		} else if _, ok := choice.(map[string]interface{}); ok {
			// Marshal the map to JSON string for the API? Doc is ambiguous.
			// Let's assume the API expects the string keywords or the specific map structure directly.
			// If gollm passes the map, we might need to adjust how ToolChoice is defined in ChatCompletionRequest
			// For now, let's try assigning the string representation if it's a map, might fail.
			// Safest might be to only support string choices here until clarified.
			log.Printf("Warning: Specific tool_choice map provided, assigning 'auto' for now.")
			req.ToolChoice = "auto"
		} else {
			// Assign string default if type is unexpected
			req.ToolChoice = "auto"
		}
	}

	// Handle Streaming flag
	if stream, ok := options["stream"].(bool); ok && stream {
		req.Stream = true
	}

	// Handle Response Format (JSON Schema)
	if schemaVal, ok := options["_schema_internal"]; ok {
		if schemaPtr, ok := schemaVal.(*jsonschema.Schema); ok {
			req.ResponseFormat = &struct {
				Type       string `json:"type"`
				JSONSchema *struct {
					Name   string             `json:"name"`
					Schema *jsonschema.Schema `json:"schema"`
					Strict bool               `json:"strict,omitempty"`
				} `json:"json_schema,omitempty"`
			}{
				Type: "json_schema",
				JSONSchema: &struct {
					Name   string             `json:"name"`
					Schema *jsonschema.Schema `json:"schema"`
					Strict bool               `json:"strict,omitempty"`
				}{
					Name:   "structured_output",
					Schema: schemaPtr,
				},
			}
		} else {
			p.logger.Error("Schema provided in options is not *jsonschema.Schema", "type", fmt.Sprintf("%T", schemaVal))
		}
	}

	p.logger.Debug("Preparing request", "provider", p.Name(), "model", req.Model, "streaming", req.Stream, "has_tools", len(req.Tools) > 0)
	if apiKey == "" {
		p.logger.Warn("API key is not set for Cerebras provider")
	}

	return json.Marshal(req)
}

// PrepareRequestWithSchema uses the ResponseFormat field.
func (p *CerebrasProvider) PrepareRequestWithSchema(prompt string, options map[string]interface{}, schema interface{}) ([]byte, error) {
	if !p.SupportsJSONSchema() {
		return nil, errors.New("internal error: PrepareRequestWithSchema called but SupportsJSONSchema is false")
	}

	var schemaPtr *jsonschema.Schema
	if s, ok := schema.(*jsonschema.Schema); ok {
		schemaPtr = s
	} else {
		reflector := jsonschema.Reflector{}
		schemaPtr = reflector.Reflect(schema)
		if schemaPtr == nil {
			return nil, fmt.Errorf("failed to reflect jsonschema from provided schema type: %T", schema)
		}
	}

	if options == nil {
		options = make(map[string]interface{})
	}
	options["_schema_internal"] = schemaPtr

	return p.PrepareRequest(prompt, options)
}

// PrepareRequestWithMessages handles messages and tools.
// Using gollm_types.MemoryMessage now
func (p *CerebrasProvider) PrepareRequestWithMessages(messages []gollm_types.MemoryMessage, options map[string]interface{}) ([]byte, error) {
	p.mutex.Lock()
	model := p.model
	maxTokens := p.maxTokens
	apiKey := p.apiKey
	temp := p.temperature
	topP := p.topP
	seed := p.seed
	p.mutex.Unlock()

	req := ChatCompletionRequest{
		Model:       model,
		Messages:    convertMessagesToCerebras(messages), // Use helper
		MaxTokens:   maxTokens,
		Temperature: temp,
		TopP:        topP,
		Seed:        seed,
	}

	// Apply Options (same logic as PrepareRequest)
	if m, ok := options["model"].(string); ok && m != "" {
		req.Model = m
	}
	if mtVal, ok := options["max_tokens"]; ok {
		switch mt := mtVal.(type) {
		case int:
			if mt > 0 {
				req.MaxTokens = mt
			}
		case float64:
			if mt > 0 {
				req.MaxTokens = int(mt)
			}
		}
	}
	if tVal, ok := options["temperature"].(float64); ok {
		req.Temperature = &tVal
	}
	if pVal, ok := options["top_p"].(float64); ok {
		req.TopP = &pVal
	}
	if sVal, ok := options["seed"]; ok {
		switch s := sVal.(type) {
		case int:
			seed64 := int64(s)
			req.Seed = &seed64
		case int64:
			req.Seed = &s
		case float64:
			seed64 := int64(s)
			req.Seed = &seed64
		}
	}
	if stopVal, ok := options["stop"].([]string); ok {
		req.Stop = stopVal
	}

	// Handle Tools & ToolChoice
	if toolsVal, ok := options["tools"].([]utils.Tool); ok {
		apiTools, err := convertToolsToCerebras(toolsVal)
		if err != nil {
			p.logger.Error("Failed to convert tools", "error", err)
		}
		req.Tools = apiTools
	}
	if toolChoiceVal, ok := options["tool_choice"]; ok {
		choice := getCerebrasToolChoice(toolChoiceVal)
		if choiceStr, ok := choice.(string); ok {
			req.ToolChoice = choiceStr
		} else {
			// Handle map case - assuming string for now based on ambiguity
			log.Printf("Warning: Specific tool_choice map provided, assigning 'auto' for now.")
			req.ToolChoice = "auto"
		}
	}

	// Handle Streaming flag
	if stream, ok := options["stream"].(bool); ok && stream {
		req.Stream = true
	}

	// Handle Response Format (JSON Schema)
	if schemaVal, ok := options["_schema_internal"]; ok {
		if schemaPtr, ok := schemaVal.(*jsonschema.Schema); ok {
			req.ResponseFormat = &struct {
				Type       string `json:"type"`
				JSONSchema *struct {
					Name   string             `json:"name"`
					Schema *jsonschema.Schema `json:"schema"`
					Strict bool               `json:"strict,omitempty"`
				} `json:"json_schema,omitempty"`
			}{
				Type: "json_schema",
				JSONSchema: &struct {
					Name   string             `json:"name"`
					Schema *jsonschema.Schema `json:"schema"`
					Strict bool               `json:"strict,omitempty"`
				}{
					Name:   "structured_output",
					Schema: schemaPtr,
				},
			}
		} else {
			p.logger.Error("Schema provided in options is not *jsonschema.Schema", "type", fmt.Sprintf("%T", schemaVal))
		}
	}

	p.logger.Debug("Preparing request with messages", "provider", p.Name(), "model", req.Model, "streaming", req.Stream, "num_msgs", len(req.Messages), "has_tools", len(req.Tools) > 0)
	if apiKey == "" {
		p.logger.Warn("API key is not set for Cerebras provider")
	}

	return json.Marshal(req)
}

// ParseResponse extracts the generated text or handles tool calls (for non-streaming).
func (p *CerebrasProvider) ParseResponse(body []byte) (string, error) {
	var response ChatCompletionResponse
	if err := json.Unmarshal(body, &response); err != nil {
		p.logger.Error("Failed to unmarshal Cerebras response", "error", err, "body", string(body))
		return "", fmt.Errorf("failed to unmarshal Cerebras response: %w", err)
	}

	if len(response.Choices) == 0 {
		p.logger.Warn("No choices found in Cerebras response", "body", string(body))
		var errResp map[string]interface{}
		if err := json.Unmarshal(body, &errResp); err == nil {
			if errVal, ok := errResp["error"]; ok {
				return "", fmt.Errorf("cerebras API error: %v", errVal)
			}
		}
		return "", errors.New("no response choices returned from Cerebras")
	}

	choice := response.Choices[0]

	if choice.FinishReason == "tool_calls" {
		p.logger.Info("Tool calls requested by model (handled by HandleFunctionCalls)", "num_calls", len(choice.Message.ToolCalls))
		return "", nil // Defer to HandleFunctionCalls
	}

	return choice.Message.Content, nil
}

// SetExtraHeaders configures additional HTTP headers.
func (p *CerebrasProvider) SetExtraHeaders(extraHeaders map[string]string) {
	p.mutex.Lock()
	defer p.mutex.Unlock()
	if p.extraHeaders == nil {
		p.extraHeaders = make(map[string]string)
	}
	for k, v := range extraHeaders {
		p.extraHeaders[k] = v
	}
	p.logger.Debug("Extra headers set", "headers", p.extraHeaders)
}

// HandleFunctionCalls processes tool calls from the response body.
func (p *CerebrasProvider) HandleFunctionCalls(body []byte) ([]byte, error) {
	var response ChatCompletionResponse
	if err := json.Unmarshal(body, &response); err != nil {
		return body, nil // Not a response with tool calls
	}

	if len(response.Choices) > 0 && response.Choices[0].FinishReason == "tool_calls" {
		toolCalls := response.Choices[0].Message.ToolCalls
		if len(toolCalls) > 0 {
			p.logger.Info("Handling tool calls", "count", len(toolCalls))
			// Marshal the tool calls into JSON format for gollm core to process
			// We need to convert CerebrasToolCall back to a format gollm understands (like utils.ToolCall)
			gollmToolCalls := make([]utils.Tool, 0, len(toolCalls))
			for _, tc := range toolCalls {
				// Basic conversion, assuming gollm's utils.ToolCall matches structure
				gollmToolCalls = append(gollmToolCalls, utils.Tool{
					//ID:   tc.ID,
					Type: tc.Type,
					Function: utils.Function{
						Name: tc.Function.Name,
					},
				})
			}

			toolCallsJSON, err := json.Marshal(gollmToolCalls) // Marshal the gollm-compatible calls
			if err != nil {
				p.logger.Error("Failed to marshal gollm tool calls for handling", "error", err)
				return body, fmt.Errorf("failed to marshal tool calls: %w", err)
			}
			// Return the JSON of the gollm-compatible tool calls
			return toolCallsJSON, nil
		}
	}

	return body, nil // No tool calls detected
}

// SupportsJSONSchema indicates support via ResponseFormat.
func (p *CerebrasProvider) SupportsJSONSchema() bool {
	return true
}

// SetDefaultOptions applies global configuration defaults.
func (p *CerebrasProvider) SetDefaultOptions(cfg *config.Config) {
	p.mutex.Lock()
	defer p.mutex.Unlock()

	var providerModel string
	var providerAPIKey string
	var providerMaxTokens int
	var providerTemp *float64
	var providerTopP *float64
	var providerSeed *int64

	if cfg == nil {
		return
	}
	// Get provider-specific API key if available
	if cfg.APIKeys != nil {
		if apiKey, ok := cfg.APIKeys[p.Name()]; ok {
			providerAPIKey = apiKey
		}
	}

	// Apply provider-specific defaults
	if providerModel != "" && (p.model == "" || p.model == "llama-4-scout-17b-16e-instruct") {
		p.model = providerModel
	}
	if providerAPIKey != "" && p.apiKey == "" {
		p.apiKey = providerAPIKey
	}
	if providerMaxTokens > 0 && (p.maxTokens == 0 || p.maxTokens == 1000) {
		p.maxTokens = providerMaxTokens
	}
	if p.temperature == nil {
		p.temperature = providerTemp
	}
	if p.topP == nil {
		p.topP = providerTopP
	}
	if p.seed == nil {
		p.seed = providerSeed
	}

	// Fallback to global defaults
	if (p.model == "" || p.model == "llama-4-scout-17b-16e-instruct") && cfg.Model != "" {
		p.model = cfg.Model
	}
	if (p.maxTokens == 0 || p.maxTokens == 1000) && cfg.MaxTokens > 0 {
		p.maxTokens = cfg.MaxTokens
	}

	// Apply other common options via SetOption
	if p.temperature == nil && cfg.Temperature > 0 {
		p.setOptionInternal("temperature", cfg.Temperature)
	}
	if p.seed == nil && cfg.Seed != nil {
		p.setOptionInternal("seed", *cfg.Seed)
	}

	p.logger.Info("Default options applied", "model", p.model, "maxTokens", p.maxTokens)
}

// setOptionInternal is called by SetDefaultOptions/SetOption without locking
func (p *CerebrasProvider) setOptionInternal(key string, value interface{}) {
	switch key {
	case "api_key":
		if v, ok := value.(string); ok {
			p.apiKey = v
		}
	case "model":
		if v, ok := value.(string); ok {
			p.model = v
		}
	case "max_tokens":
		switch v := value.(type) {
		case int:
			if v > 0 {
				p.maxTokens = v
			}
		case float64:
			if v > 0 {
				p.maxTokens = int(v)
			}
		}
	case "temperature":
		if v, ok := value.(float64); ok {
			p.temperature = &v
		}
	case "top_p":
		if v, ok := value.(float64); ok {
			p.topP = &v
		}
	case "seed":
		switch v := value.(type) {
		case int:
			seedVal := int64(v)
			p.seed = &seedVal
		case int64:
			p.seed = &v
		case float64:
			seedVal := int64(v)
			p.seed = &seedVal
		}
	default:
		p.logger.Warn("Attempted to set unknown option for CerebrasProvider", "key", key)
	}
}

// SetOption sets a specific configuration option.
func (p *CerebrasProvider) SetOption(key string, value interface{}) {
	p.mutex.Lock()
	defer p.mutex.Unlock()
	p.logger.Debug("Setting option", "key", key)
	p.setOptionInternal(key, value)
}

// SetLogger configures the logger for the provider.
func (p *CerebrasProvider) SetLogger(logger utils.Logger) {
	p.mutex.Lock()
	defer p.mutex.Unlock()
	if logger != nil {
		p.logger = logger
		p.logger.Debug("Logger configured for Cerebras provider")
	}
}

// SupportsStreaming indicates whether the provider supports streaming.
func (p *CerebrasProvider) SupportsStreaming() bool {
	return true
}

// PrepareStreamRequest creates a request body for streaming API calls.
func (p *CerebrasProvider) PrepareStreamRequest(prompt string, options map[string]interface{}) ([]byte, error) {
	if options == nil {
		options = make(map[string]interface{})
	}
	options["stream"] = true
	// Use PrepareRequestWithMessages if messages are available in options?
	// For now, assume simple prompt for streaming request prep
	return p.PrepareRequest(prompt, options)
}

// ParseStreamResponse processes a single chunk from a streaming response.
func (p *CerebrasProvider) ParseStreamResponse(chunk []byte) (string, error) {
	trimmedChunk := strings.TrimSpace(string(chunk))
	if trimmedChunk == "[DONE]" {
		return "", nil
	}
	if trimmedChunk == "" {
		return "", nil
	}

	var chunkData streamChunk
	if err := json.Unmarshal([]byte(trimmedChunk), &chunkData); err != nil {
		// Check for API error structure
		var apiError map[string]interface{}
		if errJson := json.Unmarshal([]byte(trimmedChunk), &apiError); errJson == nil {
			if errVal, ok := apiError["error"]; ok {
				p.logger.Error("Received error structure in stream", "error", errVal)
				if errMap, ok := errVal.(map[string]interface{}); ok {
					if msg, ok := errMap["message"].(string); ok {
						return "", fmt.Errorf("streaming API error: %s", msg)
					}
				}
				return "", fmt.Errorf("received unknown error structure in stream: %v", apiError)
			}
		}
		p.logger.Error("Failed to unmarshal stream chunk", "error", err, "chunk", trimmedChunk)
		return "", fmt.Errorf("failed to unmarshal stream chunk: %w", err)
	}

	if len(chunkData.Choices) == 0 {
		return "", nil
	}

	choice := chunkData.Choices[0]

	// Handle streaming tool calls - needs clarification on gollm's expected handling
	if len(choice.Delta.ToolCalls) > 0 {
		// Gollm might expect tool calls to be aggregated and handled separately from text content.
		// We could potentially marshal the delta tool calls and return them, but ParseStreamResponse
		// is expected to return the text chunk. Returning empty string for now.
		p.logger.Info("Received tool call delta in stream", "count", len(choice.Delta.ToolCalls))
		// toolCallsJSON, _ := json.Marshal(choice.Delta.ToolCalls) // Example: Marshal if needed
		// return string(toolCallsJSON), nil // Or return special marker?
		return "", nil // Return empty for non-text content
	}

	if choice.FinishReason != nil {
		p.logger.Debug("Stream finished", "reason", *choice.FinishReason)
		return choice.Delta.Content, nil // Return final content delta
	}

	return choice.Delta.Content, nil // Return normal content delta
}

// --- Compile-time Interface Check ---
var _ providers.Provider = (*CerebrasProvider)(nil)

const (
	CerebrasAPIURL = "https://api.cerebras.ai/v1/chat/completions"
)

// CerebrasClient represents a client for the Cerebras API
type CerebrasClient struct {
	// No internal state like apiKey, model, isRunning needed here anymore.
	// The http.Client is the main state.
	client *http.Client
}

// Message struct remains the same
type Message struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

// NewCerebrasClient creates a new instance of CerebrasClient
func NewCerebrasClient() *CerebrasClient {
	// Only initialize the http client
	return &CerebrasClient{
		client: &http.Client{},
	}
}

// convertToCerebrasMessages converts a slice of Message to CerebrasMessage
func convertToCerebrasMessages(messages []Message) []CerebrasMessage {
	cerebrasMessages := make([]CerebrasMessage, len(messages))
	for i, msg := range messages {
		cerebrasMessages[i] = CerebrasMessage{
			Role:    msg.Role,
			Content: msg.Content,
		}
	}
	return cerebrasMessages
}

// MakeChatCompletionRequest performs the actual API call to Cerebras.
// It takes configuration parameters for each request.
func (c *CerebrasClient) MakeChatCompletionRequest(ctx context.Context, apiKey, model string, messages []Message, maxTokens int) (string, error) {
	if apiKey == "" {
		return "", errors.New("cerebras API key is required")
	}
	if model == "" {
		return "", errors.New("cerebras model is required")
	}
	if len(messages) == 0 {
		return "", errors.New("messages cannot be empty")
	}

	// Convert messages to CerebrasMessage format
	cerebrasMessages := convertToCerebrasMessages(messages)

	// Create the request body
	requestBody := ChatCompletionRequest{
		Model:     model,
		Messages:  cerebrasMessages,
		MaxTokens: maxTokens,
		// Stream: false, // Assuming non-streaming
	}

	// Convert the request body to JSON
	requestJSON, err := json.Marshal(requestBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create the HTTP request with context
	req, err := http.NewRequestWithContext(ctx, "POST", CerebrasAPIURL, bytes.NewBuffer(requestJSON))
	if err != nil {
		return "", fmt.Errorf("failed to create request: %w", err)
	}

	// Set the headers
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+apiKey)
	req.Header.Set("User-Agent", "FIG-Inference/1.0 (via Gollm Provider)") // Identify source

	// Send the request using the client's http.Client
	resp, err := c.client.Do(req)
	if err != nil {
		// Check for context cancellation
		if errors.Is(err, context.Canceled) {
			log.Println("Cerebras request cancelled.")
			return "", err
		}
		if errors.Is(err, context.DeadlineExceeded) {
			log.Println("Cerebras request timed out.")
			return "", err
		}
		return "", fmt.Errorf("failed to send request to Cerebras API: %w", err)
	}
	defer resp.Body.Close()

	// Read the response body
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("failed to read Cerebras response body: %w", err)
	}

	// Check for non-OK status code
	if resp.StatusCode != http.StatusOK {
		log.Printf("Cerebras API Error Response Body: %s", string(body))
		return "", fmt.Errorf("cerebras API request failed with status %d: %s", resp.StatusCode, string(body))
	}

	// Parse the response
	var response ChatCompletionResponse
	if err := json.Unmarshal(body, &response); err != nil {
		return "", fmt.Errorf("failed to unmarshal Cerebras response: %w", err)
	}

	// Check if there are any choices
	if len(response.Choices) == 0 || response.Choices[0].Message.Content == "" {
		log.Printf("Cerebras response body with no choices: %s", string(body))
		return "", errors.New("no response choices or empty content returned from Cerebras")
	}

	// Return the content of the first choice
	return response.Choices[0].Message.Content, nil
}
